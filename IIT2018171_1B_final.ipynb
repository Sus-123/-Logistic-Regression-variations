{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIT2018171_1B_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOfPi-csgv6a",
        "outputId": "ad5f6ab7-e61e-4fc9-fd42-8da0f82dcd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from csv import DictReader as dr\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return (1/(1+np.exp(-x)))\n",
        "\n",
        "def computeError(predicted, actual):\n",
        "    Error = np.mean(predicted != actual)\n",
        "    return Error * 100\n",
        "\n",
        "def find_y(Y):\n",
        "    m=len(Y)\n",
        "    for i in range(m):\n",
        "       # print(Y[i])\n",
        "        if Y[i]>=0.5:\n",
        "            Y[i]=1\n",
        "        else:\n",
        "            Y[i]=0\n",
        "    return Y\n",
        "\n",
        "def denormalise_price(price):\n",
        "    global mean\n",
        "    global stddev\n",
        "    #print(price)\n",
        "    ret = price * stddev[2] + mean[2]\n",
        "    #print()\n",
        "    #print(ret)\n",
        "    return ret\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_feature(X):\n",
        "    m=X.shape[0]\n",
        "    one = np.ones((len(X),6))\n",
        "    for i in range(m):\n",
        "        x1_2=X[i][0]**2\n",
        "        x2_2=X[i][1]**2\n",
        "        x1_x2=X[i][0]*X[i][1]\n",
        "        x1_2_x2=(X[i][0]**2)*X[i][1]\n",
        "        x1_x2_2=(X[i][1]**2)*X[i][0]\n",
        "        x1_2_x2_2=(X[i][0]**2)*(X[i][1]**2)\n",
        "        one[i][0]=x1_2\n",
        "        one[i][1]=x2_2\n",
        "        one[i][2]=x1_x2\n",
        "        one[i][3]=x1_2_x2\n",
        "        one[i][4]=x1_x2_2\n",
        "        one[i][5]=x1_2_x2_2\n",
        "    X = np.concatenate((X,one),axis=1)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def batch_gradient(X_train,Y_train):\n",
        "    interations=1000\n",
        "    alpha = 0.01        \n",
        "    W0 = 0\n",
        "    W1 = 0\n",
        "    sz = len(X_train)\n",
        "            \n",
        "    costs = []\n",
        "    W = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
        "    #print(X_train.shape)\n",
        "    for epoc in range(interations):\n",
        "                \n",
        "        s=sigmoid(X_train.dot(W))\n",
        "       # print(s.shape,Y_train.shape)\n",
        "        d=(s-Y_train)\n",
        "       # print(d.shape)\n",
        "        #print(d.shape)\n",
        "        diff=X_train.T.dot(d)*(alpha/sz)\n",
        "       # print(diff.shape)\n",
        "        W = W - diff\n",
        "        new_Cost = np.sum((sigmoid(X_train.dot(W))-Y_train)**2)/(2*sz)\n",
        "        costs.append(new_Cost)\n",
        "    return (costs,W)\n",
        "\n",
        "def stochastic_gradient(X_train,Y_train):\n",
        "    interations=1000\n",
        "    alpha = 0.01        \n",
        "    W0 = 0\n",
        "    W1 = 0\n",
        "    sz = len(X_train)\n",
        "            \n",
        "    costs = []\n",
        "    W = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
        "    for epoc in range(interations):  \n",
        "        cost=0.0\n",
        "        for j in range(sz):\n",
        "            rand_int=np.random.randint(0,sz)\n",
        "            X_i=X_train[rand_int,:].reshape(1,X_train.shape[1])\n",
        "            Y_i=Y_train[rand_int]\n",
        "            pred=sigmoid(np.dot(X_i,W))\n",
        "            W=W-(1/sz)*alpha*(np.dot(np.transpose(X_i),(pred-Y_i)))\n",
        "        new_Cost = np.sum((sigmoid(X_train.dot(W))-Y_train)**2)/(2*sz)\n",
        "        costs.append(new_Cost)\n",
        "    return (costs,W)\n",
        "\n",
        "def mini_gradient(X_train,Y_train):\n",
        "    interations=1000\n",
        "    alpha = 0.01        \n",
        "    W0 = 0\n",
        "    W1 = 0\n",
        "    sz = len(X_train)\n",
        "            \n",
        "    costs = []\n",
        "    W = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
        "    #print(X_train.shape)\n",
        "    for epoc in range(interations):  \n",
        "        cost=0.0\n",
        "        num_batch=int(sz/10)\n",
        "        for i in range(1):\n",
        "            X_i=X_train[i:i+10]\n",
        "            Y_i=Y_train[i:i+10]\n",
        "            pred=sigmoid(np.dot(X_i,W))\n",
        "            W=W-(1/sz)*alpha*(np.dot(np.transpose(X_i),(pred-Y_i)))\n",
        "        new_Cost = np.sum((sigmoid(X_train.dot(W))-Y_train)**2)/(2*sz)\n",
        "        costs.append(new_Cost)\n",
        "    return (costs,W)\n",
        "\n",
        "data=pd.read_csv('data.csv')\n",
        "\n",
        "\n",
        "Y=data.iloc[:,-1:]\n",
        "X=data.iloc[:,:-1]\n",
        "Y = np.array(Y)\n",
        "X = np.array(X)\n",
        "\n",
        "X=add_feature(X)\n",
        "one = np.ones((len(X),1))\n",
        "X = np.concatenate((one,X),axis=1)\n",
        "totalsize = len(X)\n",
        "trainsize = int(totalsize*0.7)\n",
        "Y_train = Y[:trainsize]\n",
        "Y_test = Y[trainsize:]\n",
        "X_train = X[:trainsize]\n",
        "X_test = X[trainsize:]\n",
        "\n",
        "#print(X_test)\n",
        "costs,W=batch_gradient(X_train,Y_train)\n",
        "print(\"Without Feature Scaling\")\n",
        "print(\"Batch_Gradient\\nParameters:\\n\",W)\n",
        "print()\n",
        "\n",
        "\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "#print(X_test.shape,W.shape)\n",
        "Y_pred=find_y(Y_pred)\n",
        "er=computeError(Y_pred,Y_test)\n",
        "print(\"Error is :\",er,\"%\\nAccuracy is :\",100-er,\"%\")\n",
        "costs,W=stochastic_gradient(X_train,Y_train)\n",
        "print(\"\\nStochastic_Gradient\\nParameters:\\n\",W)\n",
        "\n",
        "\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "#print(X_test.shape,W.shape)\n",
        "Y_pred=find_y(Y_pred)\n",
        "er=computeError(Y_pred,Y_test)\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "costs,W=mini_gradient(X_train,Y_train)\n",
        "print(\"\\nMini_Gradient\\nParameters:\\n\",W)\n",
        "\n",
        "\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "#print(X_test.shape,W.shape)\n",
        "Y_pred=find_y(Y_pred)\n",
        "er=computeError(Y_pred,Y_test)\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "\n",
        "mean = data.mean()\n",
        "stddev = data.std()\n",
        "data = (data - data.mean())/data.std()\n",
        "X=data.iloc[:,:-1]\n",
        "Y=data.iloc[:,-1:]\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X=add_feature(X)\n",
        "one = np.ones((len(X),1))\n",
        "X = np.concatenate((one,X),axis=1)\n",
        "totalsize = len(X)\n",
        "trainsize = int(totalsize*0.7)\n",
        "X_train = X[:trainsize]\n",
        "X_test = X[trainsize:]\n",
        "Y_train = Y[:trainsize]\n",
        "Y_test = Y[trainsize:]\n",
        "costs,W=batch_gradient(X_train,Y_train)\n",
        "print(\"\\nWith Feature Scaling\")\n",
        "print(\"Batch_Gradient | Parameters  :\\n\",W)\n",
        "print()\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "Y_pred=denormalise_price(Y_pred)\n",
        "\n",
        "\n",
        "Y_pred=find_y(Y_pred)\n",
        "#print(Y_pred,Y_test)\n",
        "er=computeError(Y_pred,denormalise_price(Y_test))\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "costs,W=stochastic_gradient(X_train,Y_train)\n",
        "print(\"\\nStochastic_Gradient | Parameters:\\n\",W)\n",
        "print()\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "Y_pred=denormalise_price(Y_pred)\n",
        "Y_pred=find_y(Y_pred)\n",
        "\n",
        "\n",
        "#print(Y_pred,Y_test)\n",
        "er=computeError(Y_pred,denormalise_price(Y_test))\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "costs,W=stochastic_gradient(X_train,Y_train)\n",
        "print(\"\\nMini_Gradient | Parameters:\\n\",W)\n",
        "print()\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "Y_pred=denormalise_price(Y_pred)\n",
        "Y_pred=find_y(Y_pred)\n",
        "\n",
        "\n",
        "#print(Y_pred,Y_test)\n",
        "er=computeError(Y_pred,denormalise_price(Y_test))\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Without Feature Scaling\n",
            "Batch_Gradient\n",
            "Parameters:\n",
            " [[-1.95628571e+00]\n",
            " [-1.28832420e+02]\n",
            " [-1.45825527e+02]\n",
            " [-6.02256550e+03]\n",
            " [-7.64363689e+03]\n",
            " [-5.30775940e+03]\n",
            " [-1.67378999e+05]\n",
            " [-1.98134476e+05]\n",
            " [ 1.47766082e+05]]\n",
            "\n",
            "Error is : 20.0 %\n",
            "Accuracy is : 80.0 %\n",
            "\n",
            "Stochastic_Gradient\n",
            "Parameters:\n",
            " [[-5.11285714e-01]\n",
            " [-6.65139531e+01]\n",
            " [-7.20980015e+01]\n",
            " [-3.19358288e+03]\n",
            " [-3.62176912e+03]\n",
            " [-2.69702041e+03]\n",
            " [-8.90296129e+04]\n",
            " [-9.44769371e+04]\n",
            " [ 2.44519560e+03]]\n",
            "Error: 40.0 %\n",
            "Accuracy: 60.0 %\n",
            "\n",
            "Mini_Gradient\n",
            "Parameters:\n",
            " [[ 6.00571429e-01]\n",
            " [-9.92446933e+00]\n",
            " [-2.31487329e+01]\n",
            " [-1.38520980e+02]\n",
            " [-1.49591552e+03]\n",
            " [-6.31191344e+02]\n",
            " [-5.48552708e+03]\n",
            " [-3.57749046e+04]\n",
            " [ 9.32878535e+03]]\n",
            "Error: 20.0 %\n",
            "Accuracy: 80.0 %\n",
            "\n",
            "With Feature Scaling\n",
            "Batch_Gradient | Parameters  :\n",
            " [[-2.92132139]\n",
            " [ 6.30532193]\n",
            " [ 4.78147166]\n",
            " [-5.37483094]\n",
            " [-3.55299645]\n",
            " [ 0.63168983]\n",
            " [ 3.06224537]\n",
            " [ 3.58763317]\n",
            " [-6.26313748]]\n",
            "\n",
            "Error: 20.0 %\n",
            "Accuracy: 80.0 %\n",
            "\n",
            "Stochastic_Gradient | Parameters:\n",
            " [[-2.96273984]\n",
            " [ 6.33670876]\n",
            " [ 4.8146394 ]\n",
            " [-5.42600994]\n",
            " [-3.57319534]\n",
            " [ 0.64420747]\n",
            " [ 3.07522532]\n",
            " [ 3.59956262]\n",
            " [-6.28695897]]\n",
            "\n",
            "Error: 20.0 %\n",
            "Accuracy: 80.0 %\n",
            "\n",
            "Mini_Gradient | Parameters:\n",
            " [[-2.94677808]\n",
            " [ 6.27386859]\n",
            " [ 4.86219074]\n",
            " [-5.37739054]\n",
            " [-3.59199878]\n",
            " [ 0.6435895 ]\n",
            " [ 3.15170433]\n",
            " [ 3.49335464]\n",
            " [-6.28059138]]\n",
            "\n",
            "Error: 20.0 %\n",
            "Accuracy: 80.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}