{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIT2018171_1A_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN86FHpRV5Oi",
        "outputId": "dfe2af15-5f0e-4384-b495-87fc73571878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "import numpy as np\n",
        "from csv import DictReader \n",
        "\n",
        "\n",
        "def getG(thetax):\n",
        "    return (1 / (1 + np.exp(-1 * thetax)))\n",
        "\n",
        "def batchGradientDescent(x, y, theta, numIterations,alpha):\n",
        "    xTrans = np.transpose(x)\n",
        "    m = len(x)\n",
        "    for i in range(0, numIterations):\n",
        "        hypothesis = getG(np.dot(x, theta))\n",
        "        loss = hypothesis - y\n",
        "        gradient = np.dot(xTrans, loss) / m\n",
        "        theta = theta - alpha * gradient\n",
        "    return theta\n",
        "\n",
        "\n",
        "\n",
        "def extract():\n",
        "    file = open('/content/data.csv', 'r')\n",
        "    reduced_dict = DictReader(file)\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in reduced_dict:\n",
        "        x.append([1.0,float(i['Exam 1']),float(i['Exam 2 '])])\n",
        "        y.append([float(i['Pass/Fail'])])\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        if len(x_train) < 0.7 * len(x):\n",
        "            x_train.append(x[i])\n",
        "            y_train.append(y[i])\n",
        "        else:\n",
        "            x_test.append(x[i])\n",
        "            y_test.append(y[i])\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "x_train,y_train,x_test,y_test = extract()\n",
        "\n",
        "\n",
        "ones = np.ones((len(x_test),1))\n",
        "theta = [[1],[1],[1]]\n",
        "\n",
        "\n",
        "\n",
        "num_iterations = 1000\n",
        "alpha =0.0000001\n",
        "print(\"without feature scaling Batch Gradient Descent is :  \")\n",
        "theta= batchGradientDescent(x_train,y_train,theta,num_iterations,alpha)\n",
        "print(theta)\n",
        "\n",
        "y_pred = getG(np.dot(x_test,theta)) \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if(y_pred[i]>= 0.5):\n",
        "        y_pred[i] = 1\n",
        "    else:\n",
        "        y_pred[i] = 0\n",
        "error = 0 \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] != y_test[i]:\n",
        "        print(y_pred[i],y_test[i])\n",
        "        error = error+1\n",
        "error = error/len(y_test)*100\n",
        "\n",
        "print(\"And the  %age error is : \",error,\"%\")\n",
        "\n",
        "def featureScaling(x_train):\n",
        "\n",
        "    min_x1 = x_train[0][1]\n",
        "    min_x2 = x_train[0][2]\n",
        "\n",
        "    max_x1 = x_train[0][1]\n",
        "    max_x2 = x_train[0][2]\n",
        "\n",
        "    x_train_scaled = x_train\n",
        "\n",
        "    avg_x1 = 0\n",
        "    avg_x2 = 0\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        min_x1 = min(x_train[i][1],min_x1)\n",
        "        min_x2 = min(x_train[i][2],min_x2)\n",
        "\n",
        "        avg_x1 += x_train[i][1]\n",
        "        avg_x2 += x_train[i][2]\n",
        "\n",
        "        max_x1 = max(x_train[i][1],max_x1)\n",
        "        max_x2 = max(x_train[i][2],max_x2) \n",
        "\n",
        "    avg_x1 = avg_x1/len(x_train)\n",
        "    avg_x2 = avg_x2/len(x_train)\n",
        "\n",
        "    for i in range(len(x_train_scaled)):\n",
        "        x_train_scaled[i][1] = (x_train_scaled[i][1] - avg_x1)/(max_x1-min_x1)\n",
        "        x_train_scaled[i][2] = (x_train_scaled[i][2] - avg_x2)/(max_x2-min_x2)\n",
        "    return x_train_scaled\n",
        "\n",
        "\n",
        "print(\"with feature scaling Batch Gradient Descent  is : \")\n",
        "x_train_scaled = featureScaling(x_train)\n",
        "theta = [[1],[1],[1]]\n",
        "print(x_train_scaled)\n",
        "\n",
        "theta= batchGradientDescent(x_train_scaled,y_train,theta,num_iterations,alpha)\n",
        "print(theta)\n",
        "\n",
        "y_pred = getG(np.dot(x_test,theta)) \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if(y_pred[i]>= 0.5):\n",
        "        y_pred[i] = 1\n",
        "    else:\n",
        "        y_pred[i] = 0\n",
        "error = 0 \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] != y_test[i]:\n",
        "        print(y_pred[i],y_test[i])\n",
        "        error = error+1\n",
        "error = error/len(y_test)*100\n",
        "print(\"And the  %age error is \",error,\"%\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "without feature scaling Batch Gradient Descent is :  \n",
            "[[0.99995143]\n",
            " [0.99749146]\n",
            " [0.99731272]]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "And the  %age error is :  20.0 %\n",
            "with feature scaling Batch Gradient Descent  is : \n",
            "[[1.0, -0.42248191383243333, 0.20129978609183655], [1.0, -0.48514217573552515, -0.29865054135621977], [1.0, -0.40480117908208313, 0.12626269815125174], [1.0, -0.053206205184509116, 0.3226462593950899], [1.0, 0.2191406775841619, 0.1620370549110696], [1.0, -0.2713612948945694, -0.11669549974054327], [1.0, -0.039855306588017794, 0.4721035001464361], [1.0, 0.16123320810088704, -0.2596998294262299], [1.0, 0.17675096051811776, 0.33893569791366374], [1.0, 0.29716112094246266, -0.30394751636894696], [1.0, 0.46228355147646827, -0.3817036736988813], [1.0, 0.1610730204605449, -0.4933550897627231], [1.0, 0.2664480330363781, 0.17870104555494756], [1.0, 0.07945522998911655, 0.4897881898779887], [1.0, -0.35147459588978297, 0.1721802105112837], [1.0, -0.14295067539697826, 0.3651094286840283], [1.0, 0.07520109127176063, -0.1690772565978825], [1.0, 0.058971811413974756, -0.257875198493805], [1.0, 0.09819314901460031, 0.4195989014819224], [1.0, 0.18946516266365213, -0.24472974874900577], [1.0, 0.05066669417861423, -0.3141275947950254], [1.0, 0.37292582432595656, 0.022216552910583096], [1.0, -0.1925978353895913, -0.2259818413602838], [1.0, -0.42842869334802675, -0.2940431191376396], [1.0, 0.20312297403355445, 0.0686962945356934], [1.0, -0.02303278366791523, 0.08308259925866789], [1.0, 0.2358634452599544, -0.2850767408056723], [1.0, 0.4225924774234414, -0.37327501483543446], [1.0, -0.029401575127811438, -0.20546953916852284], [1.0, -0.3623472308058559, 0.010443738638884127], [1.0, -0.035916420321949166, 0.12488118346348773], [1.0, 0.31120023989008067, -0.10591985240375654], [1.0, -0.1698683564255722, -0.016920578435232758], [1.0, -0.17077243980718168, 0.07544195073800272], [1.0, -0.3413818750153808, 0.10085553166999167], [1.0, -0.13335642805840553, -0.17679094304958543], [1.0, -0.4327134065017012, 0.506644910237277], [1.0, 0.004504756972620073, 0.2435369243989188], [1.0, 0.1578308095185652, -0.3326583406020309], [1.0, -0.4288393193969216, 0.16047469838691575], [1.0, 0.2894975187124552, -0.11681745706865884], [1.0, -0.17796298832266838, -0.25527191573360675], [1.0, 0.441793573217758, 0.018840879256604142], [1.0, 0.2673394806256027, -0.3466500444705917], [1.0, -0.185186536083278, -0.27041248626798964], [1.0, -0.023731170069324632, -0.17903061643947119], [1.0, 0.19256066523224968, 0.09046168660864651], [1.0, 0.48987988119686254, 0.3287879576122937], [1.0, -0.025892764527012318, 0.4758740094709818], [1.0, 0.4002065307147558, 0.3576231793278754], [1.0, 0.23231844197132243, 0.14473335900184067], [1.0, 0.5115652925077642, -0.04810136229547026], [1.0, 0.3854946987495379, -0.30603920387316563], [1.0, -0.4239143724870434, -0.056929871611877556], [1.0, -0.19618516208833411, -0.21208436849609144], [1.0, -0.2062961495764005, -0.06553430113242752], [1.0, 0.48805993352975846, 0.067073422387943], [1.0, -0.4520491717241323, 0.4587311630659255], [1.0, 0.15002077256040475, 0.08117994798609252], [1.0, 0.114590949400042, 0.20758209860814444], [1.0, 0.16659148075492736, 0.3146098423846059], [1.0, -0.41291078619265464, -0.25286627786111665], [1.0, -0.10996926850436459, -0.36652493418703586], [1.0, -0.48843470749223594, -0.21518348673022128], [1.0, -0.2773574449795562, 0.03174865638254338], [1.0, 0.038947438778343176, -0.3397089358280852], [1.0, -0.3381938179440567, 0.48710008736318733], [1.0, -0.2137240716022842, -0.18163483641348482], [1.0, 0.23715499959405076, 0.40771780356507065], [1.0, 0.041632226442810706, -0.04821323278299293]]\n",
            "[[0.99997894]\n",
            " [1.00000712]\n",
            " [1.0000051 ]]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "And the  %age error is  20.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNu0IlaKTIwK",
        "outputId": "e602e7e4-5106-4d74-d4f7-08b3b24ecafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "import numpy as np\n",
        "from csv import DictReader \n",
        "\n",
        "def extract():\n",
        "    file = open('/content/data.csv', 'r')\n",
        "    reduced_dict = DictReader(file)\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in reduced_dict:\n",
        "        x.append([1.0,float(i['Exam 1']),float(i['Exam 2 '])])\n",
        "        y.append([float(i['Pass/Fail'])])\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        if len(x_train) < 0.7 * len(x):\n",
        "            x_train.append(x[i])\n",
        "            y_train.append(y[i])\n",
        "        else:\n",
        "            x_test.append(x[i])\n",
        "            y_test.append(y[i])\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "def getG(thetax):\n",
        "    return (1 / (1 + np.exp(-1 * thetax)))\n",
        "\n",
        "def miniBatchGradientDescent(x, y, theta, numIterations,alpha):\n",
        "    batchSize = 10\n",
        "    # xTrans = np.transpose(x)\n",
        "    m = len(x)\n",
        "    for i in range(0, numIterations):\n",
        "        for j in range(0,(int)(len(x)/batchSize)):\n",
        "            start = batchSize*j\n",
        "            end = batchSize*j + batchSize\n",
        "            X = x[start:end]\n",
        "            Y = y[start:end]\n",
        "            xTrans = np.transpose(X)\n",
        "            hypothesis = getG(np.dot(X, theta))\n",
        "            loss = hypothesis - Y\n",
        "            gradient = np.dot(xTrans, loss) / m\n",
        "            theta = theta - alpha * gradient\n",
        "    return theta\n",
        "\n",
        "x_train,y_train,x_test,y_test = extract()\n",
        "\n",
        "ones = np.ones((len(x_test),1))\n",
        "theta = [[1],[1],[1]]\n",
        "\n",
        "num_iterations = 1000\n",
        "alpha =0.001\n",
        "print(\"Mini Batch Gradient Descent (without feature scaling)\")\n",
        "theta= miniBatchGradientDescent(x_train,y_train,theta,num_iterations,alpha)\n",
        "print(theta)\n",
        "\n",
        "y_pred = getG(np.dot(x_test,theta)) \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if(y_pred[i]>= 0.5):\n",
        "        y_pred[i] = 1\n",
        "    else:\n",
        "        y_pred[i] = 0\n",
        "error = 0 \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] != y_test[i]:\n",
        "        print(y_pred[i],y_test[i])\n",
        "        error = error+1\n",
        "error = error/len(y_test)*100\n",
        "\n",
        "print(\"Error %age: \",error,\"%\")\n",
        "\n",
        "def featureScaling(x_train):\n",
        "\n",
        "    min_x1 = x_train[0][1]\n",
        "    min_x2 = x_train[0][2]\n",
        "\n",
        "    max_x1 = x_train[0][1]\n",
        "    max_x2 = x_train[0][2]\n",
        "\n",
        "    x_train_scaled = x_train\n",
        "\n",
        "    avg_x1 = 0\n",
        "    avg_x2 = 0\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        min_x1 = min(x_train[i][1],min_x1)\n",
        "        min_x2 = min(x_train[i][2],min_x2)\n",
        "        # min_x3 = min(x_train[i][3],min_x3)\n",
        "\n",
        "        avg_x1 += x_train[i][1]\n",
        "        avg_x2 += x_train[i][2]\n",
        "        # avg_x3 += x_train[i][3]\n",
        "\n",
        "        max_x1 = max(x_train[i][1],max_x1)\n",
        "        max_x2 = max(x_train[i][2],max_x2)\n",
        "        # max_x3 = max(x_train[i][3],max_x3) \n",
        "\n",
        "    avg_x1 = avg_x1/len(x_train)\n",
        "    avg_x2 = avg_x2/len(x_train)\n",
        "    # avg_x3 = avg_x3/len(x_train)\n",
        "\n",
        "    for i in range(len(x_train_scaled)):\n",
        "        x_train_scaled[i][1] = (x_train_scaled[i][1] - avg_x1)/(max_x1-min_x1)\n",
        "        x_train_scaled[i][2] = (x_train_scaled[i][2] - avg_x2)/(max_x2-min_x2)\n",
        "        # x_train_scaled[i][3] = (x_train_scaled[i][3] - avg_x3)/(max_x3-min_x3)\n",
        "    return x_train_scaled\n",
        "\n",
        "\n",
        "print(\"Mini Batch Gradient Descent (with feature scaling)\")\n",
        "x_train_scaled = featureScaling(x_train)\n",
        "theta = [[1],[1],[1]]\n",
        "# num_iterations = 100\n",
        "# alpha =0.01\n",
        "# print(x_train)\n",
        "print(x_train_scaled)\n",
        "\n",
        "theta= miniBatchGradientDescent(x_train_scaled,y_train,theta,num_iterations,alpha)\n",
        "print(theta)\n",
        "\n",
        "y_pred = getG(np.dot(x_test,theta)) \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if(y_pred[i]>= 0.5):\n",
        "        y_pred[i] = 1\n",
        "    else:\n",
        "        y_pred[i] = 0\n",
        "error = 0 \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] != y_test[i]:\n",
        "        print(y_pred[i],y_test[i])\n",
        "        error = error+1\n",
        "error = error/len(y_test)*100\n",
        "print(\"Error %age: \",error,\"%\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mini Batch Gradient Descent (without feature scaling)\n",
            "[[ 0.89888379]\n",
            " [ 0.0052166 ]\n",
            " [-0.01304646]]\n",
            "[1.] [0.0]\n",
            "[0.] [1.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[0.] [1.0]\n",
            "Error %age:  26.666666666666668 %\n",
            "Mini Batch Gradient Descent (with feature scaling)\n",
            "[[1.0, -0.42248191383243333, 0.20129978609183655], [1.0, -0.48514217573552515, -0.29865054135621977], [1.0, -0.40480117908208313, 0.12626269815125174], [1.0, -0.053206205184509116, 0.3226462593950899], [1.0, 0.2191406775841619, 0.1620370549110696], [1.0, -0.2713612948945694, -0.11669549974054327], [1.0, -0.039855306588017794, 0.4721035001464361], [1.0, 0.16123320810088704, -0.2596998294262299], [1.0, 0.17675096051811776, 0.33893569791366374], [1.0, 0.29716112094246266, -0.30394751636894696], [1.0, 0.46228355147646827, -0.3817036736988813], [1.0, 0.1610730204605449, -0.4933550897627231], [1.0, 0.2664480330363781, 0.17870104555494756], [1.0, 0.07945522998911655, 0.4897881898779887], [1.0, -0.35147459588978297, 0.1721802105112837], [1.0, -0.14295067539697826, 0.3651094286840283], [1.0, 0.07520109127176063, -0.1690772565978825], [1.0, 0.058971811413974756, -0.257875198493805], [1.0, 0.09819314901460031, 0.4195989014819224], [1.0, 0.18946516266365213, -0.24472974874900577], [1.0, 0.05066669417861423, -0.3141275947950254], [1.0, 0.37292582432595656, 0.022216552910583096], [1.0, -0.1925978353895913, -0.2259818413602838], [1.0, -0.42842869334802675, -0.2940431191376396], [1.0, 0.20312297403355445, 0.0686962945356934], [1.0, -0.02303278366791523, 0.08308259925866789], [1.0, 0.2358634452599544, -0.2850767408056723], [1.0, 0.4225924774234414, -0.37327501483543446], [1.0, -0.029401575127811438, -0.20546953916852284], [1.0, -0.3623472308058559, 0.010443738638884127], [1.0, -0.035916420321949166, 0.12488118346348773], [1.0, 0.31120023989008067, -0.10591985240375654], [1.0, -0.1698683564255722, -0.016920578435232758], [1.0, -0.17077243980718168, 0.07544195073800272], [1.0, -0.3413818750153808, 0.10085553166999167], [1.0, -0.13335642805840553, -0.17679094304958543], [1.0, -0.4327134065017012, 0.506644910237277], [1.0, 0.004504756972620073, 0.2435369243989188], [1.0, 0.1578308095185652, -0.3326583406020309], [1.0, -0.4288393193969216, 0.16047469838691575], [1.0, 0.2894975187124552, -0.11681745706865884], [1.0, -0.17796298832266838, -0.25527191573360675], [1.0, 0.441793573217758, 0.018840879256604142], [1.0, 0.2673394806256027, -0.3466500444705917], [1.0, -0.185186536083278, -0.27041248626798964], [1.0, -0.023731170069324632, -0.17903061643947119], [1.0, 0.19256066523224968, 0.09046168660864651], [1.0, 0.48987988119686254, 0.3287879576122937], [1.0, -0.025892764527012318, 0.4758740094709818], [1.0, 0.4002065307147558, 0.3576231793278754], [1.0, 0.23231844197132243, 0.14473335900184067], [1.0, 0.5115652925077642, -0.04810136229547026], [1.0, 0.3854946987495379, -0.30603920387316563], [1.0, -0.4239143724870434, -0.056929871611877556], [1.0, -0.19618516208833411, -0.21208436849609144], [1.0, -0.2062961495764005, -0.06553430113242752], [1.0, 0.48805993352975846, 0.067073422387943], [1.0, -0.4520491717241323, 0.4587311630659255], [1.0, 0.15002077256040475, 0.08117994798609252], [1.0, 0.114590949400042, 0.20758209860814444], [1.0, 0.16659148075492736, 0.3146098423846059], [1.0, -0.41291078619265464, -0.25286627786111665], [1.0, -0.10996926850436459, -0.36652493418703586], [1.0, -0.48843470749223594, -0.21518348673022128], [1.0, -0.2773574449795562, 0.03174865638254338], [1.0, 0.038947438778343176, -0.3397089358280852], [1.0, -0.3381938179440567, 0.48710008736318733], [1.0, -0.2137240716022842, -0.18163483641348482], [1.0, 0.23715499959405076, 0.40771780356507065], [1.0, 0.041632226442810706, -0.04821323278299293]]\n",
            "[[0.80942351]\n",
            " [1.07009055]\n",
            " [1.05013056]]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "Error %age:  20.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IQCB_wfTQLM",
        "outputId": "5aafc36d-435a-4a4a-8a4e-e57916ff44cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "import numpy as np\n",
        "from csv import DictReader\n",
        "import random \n",
        "\n",
        "def extract():\n",
        "    file = open('/content/data.csv', 'r')\n",
        "    reduced_dict = DictReader(file)\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in reduced_dict:\n",
        "        x.append([1.0,float(i['Exam 1']),float(i['Exam 2 '])])\n",
        "        y.append([float(i['Pass/Fail'])])\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        if len(x_train) < 0.7 * len(x):\n",
        "            x_train.append(x[i])\n",
        "            y_train.append(y[i])\n",
        "        else:\n",
        "            x_test.append(x[i])\n",
        "            y_test.append(y[i])\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "def getG(thetax):\n",
        "#     print(\"hi\",thetax)\n",
        "    return (1 / (1 + np.exp(-1 * thetax)))\n",
        "\n",
        "def stochasticGradientDescent(x, y, theta, numIterations,alpha):\n",
        "    m = len(x)\n",
        "    for i in range(0, numIterations):\n",
        "        for k in range(m):\n",
        "            j = random.randrange(0,m-1)\n",
        "            start = j\n",
        "            end = j+1\n",
        "            X = x[start:end]\n",
        "            Y = y[start:end]\n",
        "            xTrans = np.transpose(X)\n",
        "            hypothesis = getG(np.dot(X, theta))\n",
        "            loss = hypothesis - Y\n",
        "            gradient = np.dot(xTrans, loss) / m\n",
        "            theta = theta - alpha * gradient\n",
        "    return theta\n",
        "\n",
        "x_train,y_train,x_test,y_test = extract()\n",
        "\n",
        "ones = np.ones((len(x_test),1))\n",
        "theta = [[1],[1],[1]]\n",
        "\n",
        "num_iterations = 1000\n",
        "alpha =0.0001\n",
        "\n",
        "print(\"Stochastic Gradient Descent (without feature scaling)\")\n",
        "theta= stochasticGradientDescent(x_train,y_train,theta,num_iterations,alpha)\n",
        "print(theta)\n",
        "\n",
        "y_pred = getG(np.dot(x_test,theta)) \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if(y_pred[i]>= 0.5):\n",
        "        y_pred[i] = 1\n",
        "    else:\n",
        "        y_pred[i] = 0\n",
        "error = 0 \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] != y_test[i]:\n",
        "        print(y_pred[i],y_test[i])\n",
        "        error = error+1\n",
        "error = error/len(y_test)*100\n",
        "\n",
        "print(\"Error %age: \",error,\"%\")\n",
        "\n",
        "def featureScaling(x_train):\n",
        "\n",
        "    min_x1 = x_train[0][1]\n",
        "    min_x2 = x_train[0][2]\n",
        "\n",
        "    max_x1 = x_train[0][1]\n",
        "    max_x2 = x_train[0][2]\n",
        "\n",
        "    x_train_scaled = x_train\n",
        "\n",
        "    avg_x1 = 0\n",
        "    avg_x2 = 0\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        min_x1 = min(x_train[i][1],min_x1)\n",
        "        min_x2 = min(x_train[i][2],min_x2)\n",
        "        # min_x3 = min(x_train[i][3],min_x3)\n",
        "\n",
        "        avg_x1 += x_train[i][1]\n",
        "        avg_x2 += x_train[i][2]\n",
        "        # avg_x3 += x_train[i][3]\n",
        "\n",
        "        max_x1 = max(x_train[i][1],max_x1)\n",
        "        max_x2 = max(x_train[i][2],max_x2)\n",
        "        # max_x3 = max(x_train[i][3],max_x3) \n",
        "\n",
        "    avg_x1 = avg_x1/len(x_train)\n",
        "    avg_x2 = avg_x2/len(x_train)\n",
        "    # avg_x3 = avg_x3/len(x_train)\n",
        "\n",
        "    for i in range(len(x_train_scaled)):\n",
        "        x_train_scaled[i][1] = (x_train_scaled[i][1] - avg_x1)/(max_x1-min_x1)\n",
        "        x_train_scaled[i][2] = (x_train_scaled[i][2] - avg_x2)/(max_x2-min_x2)\n",
        "        # x_train_scaled[i][3] = (x_train_scaled[i][3] - avg_x3)/(max_x3-min_x3)\n",
        "    return x_train_scaled\n",
        "\n",
        "x_train_scaled = featureScaling(x_train)\n",
        "theta = [[1],[1],[1]]\n",
        "# num_iterations = 100\n",
        "# alpha =0.01\n",
        "# print(x_train)\n",
        "# print(x_train_scaled)\n",
        "\n",
        "print(\"Stochastic Gradient Descent (with feature scaling)\")\n",
        "theta= stochasticGradientDescent(x_train_scaled,y_train,theta,num_iterations,alpha)\n",
        "print(theta)\n",
        "\n",
        "y_pred = getG(np.dot(x_test,theta)) \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if(y_pred[i]>= 0.5):\n",
        "        y_pred[i] = 1\n",
        "    else:\n",
        "        y_pred[i] = 0\n",
        "error = 0 \n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] != y_test[i]:\n",
        "        print(y_pred[i],y_test[i])\n",
        "        error = error+1\n",
        "error = error/len(y_test)*100\n",
        "\n",
        "print(\"Error %age: \",error,\"%\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stochastic Gradient Descent (without feature scaling)\n",
            "[[ 0.97584771]\n",
            " [ 0.0057593 ]\n",
            " [-0.01394054]]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "Error %age:  20.0 %\n",
            "Stochastic Gradient Descent (with feature scaling)\n",
            "[[0.97872433]\n",
            " [1.00711376]\n",
            " [1.00520389]]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "[1.] [0.0]\n",
            "Error %age:  20.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}