{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIT2018171_1C_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfk5NzRwmjLN",
        "outputId": "6cdccbd2-d7d3-4a4e-d7a0-7b1b646a6570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from csv import DictReader as dr\n",
        "from sklearn import *\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return (1/(1+np.exp(-x)))\n",
        "\n",
        "def computeError(predicted, actual):\n",
        "    Error = np.mean(predicted != actual)\n",
        "    return Error * 100\n",
        "\n",
        "def find_y(Y):\n",
        "    m=len(Y)\n",
        "    for i in range(m):\n",
        "       # print(Y[i])\n",
        "        if Y[i]>=0.5:\n",
        "            Y[i]=1\n",
        "        else:\n",
        "            Y[i]=0\n",
        "    return Y\n",
        "\n",
        "def denormalise_price(price):\n",
        "    global mean\n",
        "    global stddev\n",
        "    #print(price)\n",
        "    ret = price * stddev[2] + mean[2]\n",
        "    #print()\n",
        "    #print(ret)\n",
        "    return ret\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_feature(X):\n",
        "    m=X.shape[0]\n",
        "    one = np.ones((len(X),6))\n",
        "    for i in range(m):\n",
        "        x1_2=X[i][0]**2\n",
        "        x2_2=X[i][1]**2\n",
        "        x1_x2=X[i][0]*X[i][1]\n",
        "        x1_2_x2=(X[i][0]**2)*X[i][1]\n",
        "        x1_x2_2=(X[i][1]**2)*X[i][0]\n",
        "        x1_2_x2_2=(X[i][0]**2)*(X[i][1]**2)\n",
        "        one[i][0]=x1_2\n",
        "        one[i][1]=x2_2\n",
        "        one[i][2]=x1_x2\n",
        "        one[i][3]=x1_2_x2\n",
        "        one[i][4]=x1_x2_2\n",
        "        one[i][5]=x1_2_x2_2\n",
        "    X = np.concatenate((X,one),axis=1)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def batch_gradient(X_train,Y_train):\n",
        "    interations=1000\n",
        "    alpha = 0.01        \n",
        "    W0 = 0\n",
        "    W1 = 0\n",
        "    sz = len(X_train)\n",
        "            \n",
        "    costs = []\n",
        "    W = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
        "    #print(X_train.shape)\n",
        "    for epoc in range(interations):\n",
        "                \n",
        "        s=sigmoid(X_train.dot(W))\n",
        "       # print(s.shape,Y_train.shape)\n",
        "        d=(s-Y_train)\n",
        "       # print(d.shape)\n",
        "        #print(d.shape)\n",
        "        diff=X_train.T.dot(d)*(alpha/sz)\n",
        "       # print(diff.shape)\n",
        "        W = W - diff\n",
        "        new_Cost = np.sum((sigmoid(X_train.dot(W))-Y_train)**2)/(2*sz)\n",
        "        costs.append(new_Cost)\n",
        "    return (costs,W)\n",
        "\n",
        "def stochastic_gradient(X_train,Y_train):\n",
        "    interations=1000\n",
        "    alpha = 0.01        \n",
        "    W0 = 0\n",
        "    W1 = 0\n",
        "    sz = len(X_train)\n",
        "            \n",
        "    costs = []\n",
        "    W = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
        "    for epoc in range(interations):  \n",
        "        cost=0.0\n",
        "        for j in range(sz):\n",
        "            rand_int=np.random.randint(0,sz)\n",
        "            X_i=X_train[rand_int,:].reshape(1,X_train.shape[1])\n",
        "            Y_i=Y_train[rand_int]\n",
        "            pred=sigmoid(np.dot(X_i,W))\n",
        "            W=W-(1/sz)*alpha*(np.dot(np.transpose(X_i),(pred-Y_i)))\n",
        "        new_Cost = np.sum((sigmoid(X_train.dot(W))-Y_train)**2)/(2*sz)\n",
        "        costs.append(new_Cost)\n",
        "    return (costs,W)\n",
        "\n",
        "def mini_gradient(X_train,Y_train):\n",
        "    interations=1000\n",
        "    alpha = 0.01        \n",
        "    W0 = 0\n",
        "    W1 = 0\n",
        "    sz = len(X_train)\n",
        "            \n",
        "    costs = []\n",
        "    W = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
        "    #print(X_train.shape)\n",
        "    for epoc in range(interations):  \n",
        "        cost=0.0\n",
        "        num_batch=int(sz/10)\n",
        "        for i in range(1):\n",
        "            X_i=X_train[i:i+10]\n",
        "            Y_i=Y_train[i:i+10]\n",
        "            pred=sigmoid(np.dot(X_i,W))\n",
        "            W=W-(1/sz)*alpha*(np.dot(np.transpose(X_i),(pred-Y_i)))\n",
        "        new_Cost = np.sum((sigmoid(X_train.dot(W))-Y_train)**2)/(2*sz)\n",
        "        costs.append(new_Cost)\n",
        "    return (costs,W)\n",
        "\n",
        "data=pd.read_csv('data.csv')\n",
        "\n",
        "\n",
        "X=data.iloc[:,:-1]\n",
        "Y=data.iloc[:,-1:]\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X=add_feature(X)\n",
        "one = np.ones((len(X),1))\n",
        "X = np.concatenate((one,X),axis=1)\n",
        "totalsize = len(X)\n",
        "trainsize = int(totalsize*0.7)\n",
        "X_train = X[:trainsize]\n",
        "X_test = X[trainsize:]\n",
        "Y_train = Y[:trainsize]\n",
        "Y_test = Y[trainsize:]\n",
        "\n",
        "\n",
        "\n",
        "#Regularization \n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test=scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(X_test)\n",
        "costs,W=batch_gradient(X_train,Y_train)\n",
        "print(\"Without Feature Scaling\")\n",
        "print(\"Batch_Gradient\\nParameters:\\n\",W)\n",
        "print()\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "#print(X_test.shape,W.shape)\n",
        "Y_pred=find_y(Y_pred)\n",
        "er=computeError(Y_pred,Y_test)\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "costs,W=stochastic_gradient(X_train,Y_train)\n",
        "print(\"\\nStochastic_Gradient\\nParameters:\\n\",W)\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "#print(X_test.shape,W.shape)\n",
        "Y_pred=find_y(Y_pred)\n",
        "er=computeError(Y_pred,Y_test)\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "costs,W=mini_gradient(X_train,Y_train)\n",
        "print(\"\\nMini_Gradient\\nParameters:\\n\",W)\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "#print(X_test.shape,W.shape)\n",
        "Y_pred=find_y(Y_pred)\n",
        "er=computeError(Y_pred,Y_test)\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "\n",
        "mean = data.mean()\n",
        "stddev = data.std()\n",
        "data = (data - data.mean())/data.std()\n",
        "X=data.iloc[:,:-1]\n",
        "Y=data.iloc[:,-1:]\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X=add_feature(X)\n",
        "one = np.ones((len(X),1))\n",
        "X = np.concatenate((one,X),axis=1)\n",
        "totalsize = len(X)\n",
        "trainsize = int(totalsize*0.7)\n",
        "X_train = X[:trainsize]\n",
        "X_test = X[trainsize:]\n",
        "Y_train = Y[:trainsize]\n",
        "Y_test = Y[trainsize:]\n",
        "costs,W=batch_gradient(X_train,Y_train)\n",
        "print(\"\\nWith Feature Scaling\")\n",
        "print(\"Batch_Gradient\\nParameters:\\n\",W)\n",
        "print()\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "Y_pred=denormalise_price(Y_pred)\n",
        "Y_pred=find_y(Y_pred)\n",
        "#print(Y_pred,Y_test)\n",
        "er=computeError(Y_pred,denormalise_price(Y_test))\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "costs,W=stochastic_gradient(X_train,Y_train)\n",
        "print(\"\\nStochastic_Gradient\\nParameters:\\n\",W)\n",
        "print()\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "Y_pred=denormalise_price(Y_pred)\n",
        "Y_pred=find_y(Y_pred)\n",
        "#print(Y_pred,Y_test)\n",
        "er=computeError(Y_pred,denormalise_price(Y_test))\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")\n",
        "costs,W=stochastic_gradient(X_train,Y_train)\n",
        "print(\"\\nMini_Gradient\\nParameters:\\n\",W)\n",
        "print()\n",
        "Y_pred=sigmoid(np.dot(X_test,W))\n",
        "Y_pred=denormalise_price(Y_pred)\n",
        "Y_pred=find_y(Y_pred)\n",
        "#print(Y_pred,Y_test)\n",
        "er=computeError(Y_pred,denormalise_price(Y_test))\n",
        "print(\"Error:\",er,\"%\\nAccuracy:\",100-er,\"%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without Feature Scaling\n",
            "Batch_Gradient\n",
            "Parameters:\n",
            " [[1.        ]\n",
            " [1.12384917]\n",
            " [0.7958006 ]\n",
            " [0.9982344 ]\n",
            " [0.73473033]\n",
            " [0.94858014]\n",
            " [0.91619961]\n",
            " [0.81555734]\n",
            " [0.81871273]]\n",
            "\n",
            "Error: 13.333333333333334 %\n",
            "Accuracy: 86.66666666666667 %\n",
            "\n",
            "Stochastic_Gradient\n",
            "Parameters:\n",
            " [[1.        ]\n",
            " [1.12240596]\n",
            " [0.79872576]\n",
            " [0.99763449]\n",
            " [0.73828728]\n",
            " [0.94818211]\n",
            " [0.91524438]\n",
            " [0.81631403]\n",
            " [0.81817698]]\n",
            "Error: 13.333333333333334 %\n",
            "Accuracy: 86.66666666666667 %\n",
            "\n",
            "Mini_Gradient\n",
            "Parameters:\n",
            " [[1.        ]\n",
            " [1.17408553]\n",
            " [0.77473775]\n",
            " [1.16123057]\n",
            " [0.78786431]\n",
            " [0.93857439]\n",
            " [1.0025269 ]\n",
            " [0.85549126]\n",
            " [0.9095692 ]]\n",
            "Error: 16.666666666666664 %\n",
            "Accuracy: 83.33333333333334 %\n",
            "\n",
            "With Feature Scaling\n",
            "Batch_Gradient\n",
            "Parameters:\n",
            " [[-2.92132139]\n",
            " [ 6.30532193]\n",
            " [ 4.78147166]\n",
            " [-5.37483094]\n",
            " [-3.55299645]\n",
            " [ 0.63168983]\n",
            " [ 3.06224537]\n",
            " [ 3.58763317]\n",
            " [-6.26313748]]\n",
            "\n",
            "Error: 20.0 %\n",
            "Accuracy: 80.0 %\n",
            "\n",
            "Stochastic_Gradient\n",
            "Parameters:\n",
            " [[-2.89909415]\n",
            " [ 6.27793859]\n",
            " [ 4.70487564]\n",
            " [-5.33352804]\n",
            " [-3.50823695]\n",
            " [ 0.6856697 ]\n",
            " [ 2.92197697]\n",
            " [ 3.53361522]\n",
            " [-6.20066336]]\n",
            "\n",
            "Error: 20.0 %\n",
            "Accuracy: 80.0 %\n",
            "\n",
            "Mini_Gradient\n",
            "Parameters:\n",
            " [[-2.90278423]\n",
            " [ 6.36031055]\n",
            " [ 4.71564028]\n",
            " [-5.35557606]\n",
            " [-3.50313943]\n",
            " [ 0.6162811 ]\n",
            " [ 2.98438017]\n",
            " [ 3.67474526]\n",
            " [-6.23153308]]\n",
            "\n",
            "Error: 20.0 %\n",
            "Accuracy: 80.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}